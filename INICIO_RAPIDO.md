# ğŸš€ GuÃ­a RÃ¡pida de Inicio - SocialGuard ML v2.0

## âš¡ Inicio RÃ¡pido en 5 Pasos

### 1ï¸âƒ£ Ejecutar el Notebook Mejorado

```bash
# Abrir train.ipynb en Google Colab o Jupyter
# Ejecutar todas las celdas en orden
```

**Lo que hace automÃ¡ticamente:**
- âœ… Carga y combina datasets de Kaggle
- âœ… Ejecuta 4 validaciones anti-leakage
- âœ… Entrena modelo con divisiÃ³n 70/15/15
- âœ… Genera baseline para monitoreo
- âœ… Exporta modelo y artefactos

**Tiempo estimado:** 5-10 minutos

---

### 2ï¸âƒ£ Verificar Validaciones Pasadas

Busca este output en el notebook:

```
======================================================================
âœ… TODAS LAS VALIDACIONES PASADAS
âœ… El modelo estÃ¡ protegido contra leakage y drift
======================================================================
```

**Si ves advertencias:**
- ğŸ” Leer detalles de la validaciÃ³n que fallÃ³
- ğŸ“– Consultar `ML_BEST_PRACTICES.md` para soluciones

---

### 3ï¸âƒ£ Descargar Artefactos Generados

El notebook genera estos archivos:

```
phishing_model_artefacts/
â”œâ”€â”€ phishing_model_rf.joblib      â† Modelo entrenado
â”œâ”€â”€ scaler.joblib                  â† Normalizador
â”œâ”€â”€ features.json                  â† Lista de features
â”œâ”€â”€ model_metrics.json             â† MÃ©tricas completas
â”œâ”€â”€ baseline_statistics.json       â† ğŸ†• Para drift detection
â””â”€â”€ drift_monitoring_example.py    â† ğŸ†• CÃ³digo de monitoreo
```

---

### 4ï¸âƒ£ Integrar en tu AplicaciÃ³n

#### Ejemplo bÃ¡sico de predicciÃ³n:

```python
import joblib
import numpy as np

# Cargar modelo y scaler
model = joblib.load('phishing_model_rf.joblib')
scaler = joblib.load('scaler.joblib')

# Nueva URL para analizar
new_url_features = np.array([[
    -1,  # Abnormal_URL
    -1,  # Prefix_Suffix
     1,  # SSLfinal_State
    -1,  # Shortining_Service
     1,  # having_At_Symbol
    -1   # having_Sub_Domain
]])

# Normalizar y predecir
new_url_scaled = scaler.transform(new_url_features)
prediction = model.predict(new_url_scaled)
probability = model.predict_proba(new_url_scaled)

print(f"PredicciÃ³n: {'ğŸš¨ Phishing' if prediction[0] == 1 else 'âœ… LegÃ­timo'}")
print(f"Probabilidad phishing: {probability[0][1]*100:.2f}%")
```

---

### 5ï¸âƒ£ Configurar Monitoreo de Drift

```python
from drift_monitoring_example import check_drift_simple

# Cada dÃ­a/semana, ejecutar:
drift_info = check_drift_simple(
    new_data=production_predictions,
    baseline_path='baseline_statistics.json'
)

if drift_info['drift_detected']:
    print(f"âš ï¸ Drift detectado: {drift_info['features_with_drift']}")
    print(f"Severidad: {drift_info['severity']}")
    # Enviar alerta al equipo
```

---

## ğŸ“š Documentos Clave (en orden de lectura)

### Para Empezar:
1. **ğŸ“„ Este archivo** - Inicio rÃ¡pido â† ESTÃS AQUÃ
2. **ğŸ““ train.ipynb** - Notebook mejorado
3. **âœ… CHECKLIST_VALIDACION.md** - Verificar que todo estÃ© bien

### Para Profundizar:
4. **ğŸ“– ML_BEST_PRACTICES.md** - GuÃ­a completa (leer primero)
5. **ğŸ“Š PIPELINE_DIAGRAMS.md** - Diagramas visuales del flujo
6. **ğŸ“ MEJORAS_ML_V2.md** - Resumen tÃ©cnico detallado

### Para ProducciÃ³n:
7. **ğŸ drift_monitoring_example.py** - CÃ³digo helper
8. **ğŸ“‹ RESUMEN_EJECUTIVO.md** - Para stakeholders

---

## ğŸ¯ Las 4 Mejoras Clave (Resumen Ultra-RÃ¡pido)

### 1ï¸âƒ£ Data Leakage Prevention
**Problema:** Scaler veÃ­a datos de test  
**SoluciÃ³n:** Scaler entrenado SOLO con training  
**ValidaciÃ³n:** âœ… Overlap = 0, media val/test â‰  0

### 2ï¸âƒ£ Test Contamination Prevention
**Problema:** Tunear con test contamina evaluaciÃ³n  
**SoluciÃ³n:** DivisiÃ³n 70/15/15 (Train/Val/Test)  
**ValidaciÃ³n:** âœ… Test tocado UNA VEZ al final

### 3ï¸âƒ£ Data/Concept Drift Detection
**Problema:** Datos cambian con el tiempo  
**SoluciÃ³n:** KS test + baseline + monitoreo  
**ValidaciÃ³n:** âœ… Drift analysis ejecutado

### 4ï¸âƒ£ Hidden Feature Leakage Detection
**Problema:** Features "mÃ¡gicas" hacen trampa  
**SoluciÃ³n:** AnÃ¡lisis de correlaciÃ³n + varianza  
**ValidaciÃ³n:** âœ… Correlaciones < 0.95

---

## ğŸ”¥ Comandos MÃ¡s Importantes

### Entrenar Modelo:
```python
# En train.ipynb, ejecutar todas las celdas
# O desde terminal:
jupyter nbconvert --execute train.ipynb
```

### Verificar Drift:
```python
python drift_monitoring_example.py
```

### Cargar Modelo:
```python
import joblib
model = joblib.load('phishing_model_rf.joblib')
scaler = joblib.load('scaler.joblib')
```

---

## ğŸš¨ Alertas Importantes

### âš ï¸ Si ves "LEAKAGE DETECTADO":
1. ğŸ” Revisar correlaciones de features
2. ğŸ“– Consultar secciÃ³n 4 de `ML_BEST_PRACTICES.md`
3. ğŸ”§ Eliminar features problemÃ¡ticas
4. ğŸ”„ Re-ejecutar notebook

### âš ï¸ Si ves "DRIFT DETECTADO":
1. ğŸ“Š Revisar features afectadas
2. ğŸ¤” Â¿Es esperado? (datos evolucionan)
3. ğŸ“… Programar re-entrenamiento si severity > MODERATE
4. ğŸ”„ Si severity = HIGH â†’ re-entrenar YA

### âš ï¸ Si Test >> Validation:
1. ğŸš¨ Posible test contamination o suerte
2. ğŸ” Verificar uso correcto de conjuntos
3. ğŸ“– Consultar secciÃ³n 2 de `ML_BEST_PRACTICES.md`

### âš ï¸ Si Accuracy > 99%:
1. ğŸš¨ Demasiado bueno para ser verdad
2. ğŸ” Investigar feature leakage
3. ğŸ“Š Revisar correlaciones
4. ğŸ¤” Validar con conocimiento del dominio

---

## ğŸ“Š MÃ©tricas Objetivo

| MÃ©trica | MÃ­nimo Aceptable | Objetivo | Excelente |
|---------|------------------|----------|-----------|
| Accuracy | 85% | 90% | 95%+ |
| Precision | 80% | 85% | 90%+ |
| Recall | 80% | 85% | 90%+ |
| F1-Score | 80% | 85% | 90%+ |
| ROC-AUC | 0.85 | 0.90 | 0.95+ |

**Diferencia Val-Test:** < 5% (si > 10% â†’ investigar overfitting)

---

## ğŸ”„ Flujo de Trabajo TÃ­pico

### Desarrollo (Primera Vez):
```
1. Ejecutar train.ipynb completo
   â†“
2. Verificar validaciones pasadas
   â†“
3. Revisar mÃ©tricas (accuracy, F1, etc.)
   â†“
4. Si OK â†’ Descargar artefactos
   â†“
5. Integrar en aplicaciÃ³n
   â†“
6. Configurar monitoreo
   â†“
7. Deploy a producciÃ³n
```

### Mantenimiento (PeriÃ³dico):
```
1. Ejecutar drift detection (semanal)
   â†“
2. Â¿Drift detectado?
   â”‚
   â”œâ”€ NO â†’ Continuar monitoreando
   â”‚
   â””â”€ SÃ â†’ Â¿Severity?
          â”‚
          â”œâ”€ LOW â†’ Watch & monitor
          â”œâ”€ MODERATE â†’ Schedule retraining
          â””â”€ HIGH â†’ Retrain NOW!
```

### Re-entrenamiento:
```
1. Recolectar datos frescos
   â†“
2. Agregar al dataset
   â†“
3. Re-ejecutar train.ipynb
   â†“
4. Comparar mÃ©tricas con v anterior
   â†“
5. A/B testing (opcional)
   â†“
6. Deploy nueva versiÃ³n
   â†“
7. Actualizar baseline
```

---

## ğŸ’¡ Tips Pro

### âœ… Mejores PrÃ¡cticas:

1. **Siempre revisar validaciones**
   - No skipear la celda de validaciones
   - Investigar cualquier warning

2. **Guardar todo**
   - Modelo, scaler, baseline, metrics
   - Versionar con fecha/version number

3. **Monitorear desde dÃ­a 1**
   - No esperar a que haya problemas
   - Revisar drift semanalmente

4. **Documentar decisiones**
   - Por quÃ© elegiste estos hiperparÃ¡metros
   - Por quÃ© eliminaste/agregaste features

5. **A/B testing para cambios grandes**
   - Nueva versiÃ³n vs versiÃ³n actual
   - Medir impacto en usuarios reales

### âŒ Errores Comunes a Evitar:

1. **Normalizar antes de split**
   ```python
   # âŒ MAL
   X_scaled = scaler.fit_transform(X)
   X_train, X_test = train_test_split(X_scaled)
   
   # âœ… BIEN
   X_train, X_test = train_test_split(X)
   X_train_scaled = scaler.fit_transform(X_train)
   X_test_scaled = scaler.transform(X_test)
   ```

2. **Tunear con test**
   ```python
   # âŒ MAL
   for param in params:
       score = model.score(X_test, y_test)
   
   # âœ… BIEN
   for param in params:
       score = model.score(X_val, y_val)
   ```

3. **Ignorar drift warnings**
   ```python
   # âŒ MAL
   if drift_detected:
       pass  # "Lo revisarÃ© despuÃ©s"
   
   # âœ… BIEN
   if drift_detected:
       log_alert()
       investigate_cause()
       plan_retraining()
   ```

---

## ğŸ†˜ Troubleshooting RÃ¡pido

### Problema: "baseline_statistics.json not found"
**SoluciÃ³n:** Ejecutar train.ipynb completo, genera automÃ¡ticamente

### Problema: "ValueError: Feature names mismatch"
**SoluciÃ³n:** Verificar que uses las mismas features que el modelo entrenado

### Problema: "Drift detectado en todas las features"
**SoluciÃ³n:** Datos de producciÃ³n muy diferentes, necesitas re-entrenar urgente

### Problema: "Accuracy muy baja en producciÃ³n"
**SoluciÃ³n:** 
1. Verificar que features se calculen igual
2. Verificar que scaler se aplique
3. Revisar drift detection

---

## ğŸ“ Recursos de Ayuda

### DocumentaciÃ³n:
- ğŸ“– **ML_BEST_PRACTICES.md** - Referencia completa
- ğŸ“Š **PIPELINE_DIAGRAMS.md** - Visualizaciones
- âœ… **CHECKLIST_VALIDACION.md** - Checklist paso a paso

### CÃ³digo:
- ğŸ““ **train.ipynb** - Notebook principal
- ğŸ **drift_monitoring_example.py** - Scripts helper

### Referencias Externas:
- [Google ML Best Practices](https://developers.google.com/machine-learning/guides/rules-of-ml)
- [Kaggle Data Leakage](https://www.kaggle.com/learn/data-leakage)
- [Evidently AI Docs](https://docs.evidentlyai.com/)

---

## ğŸ¯ Siguiente Paso Recomendado

### Si eres nuevo en el proyecto:
ğŸ‘‰ **Leer:** `ML_BEST_PRACTICES.md` (10-15 minutos)

### Si vas a entrenar:
ğŸ‘‰ **Ejecutar:** `train.ipynb` (5-10 minutos)

### Si vas a integrar:
ğŸ‘‰ **Revisar:** SecciÃ³n "Integrar en tu AplicaciÃ³n" arriba

### Si ya estÃ¡ en producciÃ³n:
ğŸ‘‰ **Configurar:** `drift_monitoring_example.py`

---

## âœ… Checklist MÃ­nimo para Empezar

- [ ] Ejecutar `train.ipynb` completo
- [ ] Verificar que todas las validaciones pasen
- [ ] Descargar artefactos generados
- [ ] Probar predicciÃ³n con modelo cargado
- [ ] Configurar drift monitoring
- [ ] Leer `ML_BEST_PRACTICES.md`

**Â¿Todos los Ã­tems marcados?** â†’ ğŸš€ Â¡Listo para producciÃ³n!

---

**VersiÃ³n:** 2.0  
**Autor:** SocialGuard ML Team  
**Ãšltima actualizaciÃ³n:** 2025-11-15

**Â¿Preguntas?** Consultar documentos listados arriba o abrir issue en GitHub.

---

## ğŸ‰ Â¡EstÃ¡s Listo!

Ahora tienes:
- âœ… Modelo protegido contra leakage
- âœ… Pipeline validado correctamente
- âœ… Sistema de monitoreo de drift
- âœ… DocumentaciÃ³n completa
- âœ… CÃ³digo de ejemplo

**Â¡Buena suerte con tu modelo! ğŸš€**
